% !TEX root = template.tex

\section{Processing Pipeline}
\label{sec:processing_architecture}
\begin{figure}
\caption{Schematic}
\label{system_scheme}
\end{figure}

In a general way, the system used consists on the main components. An scheme of the system is shown on figure \ref{system_scheme}.  CNN is an image classification technique and one of the major challenges in speech and acoustic event recognition has been how to best represent the audio signal using an image for this purpose. Two common approaches have been seen in addressing this problem. Firstly, the audio signal is converted to spectrogram images \cite{zhang2015robust}. Secondly, a mel-filter, as used in computing mel-frequency cepstral coefficients (MFCC), is used to form an image-like representation \cite{abdel2014convolutional]}.  To ensure that all images are of an equal size, the audio signal is divided into a fixed number of frames \cite{We refer this as the mel-spectrogram.}. The first component consist on a feature extraction module  from the audio signals. The second module consists on the convolutional neural network architecture. This model will output a probability of each segment to belong to a class. Finally, a post-processing module, takes for each audio input the list of probabiltiies, and produces the final predicted class for that audio.

 



CNNs run a small window over the input image atboth training and testing time, so that the weights of the networkthat looks through this window canlearn from various featuresof the input data regardless of their absolute position within theinput.Weight sharing, or to be more precise in our present situ-ation,full weight sharingrefers to the decision to use the sameweights at every positioning of the window. CNNs are also often said to be local because the individual units that are computed at a particular positioning ofthe window depend upon featuresof the local region of the image that the window currently looksupon.
 Time presents no immediate problem from the standpoint of locality. Likeother DNNs for speech, a single window of input to the CNNwill consist of a wide amount of context (9–15 frames). Asfor frequency, the conventional use of MFCCs does present amajor problem because the discrete cosine transform projectsthe spectral energies into a new basis that may not maintain lo-cality. In this paper, we shall use the log-energy computed di-rectly from the mel-frequency spectral coefficients (i.e., with noDCT), which we will denote asMFSC features. These will beused to represent each speech frame in order to describethe acoustic energy distributionin each of several different frequency bands

There exist several different alternatives to organizing theseMFSC features into maps for the CNN. First, as shown inFig. 1(b), they can be arranged as three 2-D feature maps,each of which represents MFSC features (static, delta anddelta-delta) distributed along both frequency (using the fre-quency band index) and time (using the frame number withineach context window). In this case, a two-dimensional con-volution is performed (explained below) to normalize bothfrequency and temporal variations simultaneously. 


We evaluated our models using Google’s Speech CommandsDataset  [9],  which  was  released  in  August  2017  under  aCreative  Commons  license.2The  dataset  contains  65,000one-second long utterances of 30 short words by thousands ofdifferent people, as well as background noise samples such aspink noise, white noise, and human-made sounds.  The blogpost  announcing  the  data  release  also  references  Google’sTensorFlow implementation of Sainath and Parada’s models,which provide the basis of our comparisons


The output of each must be post-processed

\section{Signals and Features}
The audio signals For  feature  extraction,   we  first  apply  a  band-pass  filterof  20Hz/4kHz  to  the  input  audio  to  reduce  noise.    For the speech regions, we generate acoustic features based on 40-dimensional log-filterbank energies computed every 10 ms over a window of 25 ms. 
    

Contiguous frames are stacked to add sufficient left and right context. The input window is asymmetric since each additional frame of future context adds 10 ms of latency to the system. For our Deep KWS system, we use 10 future frames and 30frames in the past.


Frames with size $40x40$

No frames the every data has a a shape of 100x40
NFFT=256, nfilt=40
numcep=40, nfilt=nfilt, nfft=51

\section{Learning Framework}
 
 Basically we used two different approaches, one based on a Residual Neural Network (ResNet) proposed by Sainath \cite{sainath2013deep}, \cite{tang2018deep} and the other based on a Vision Transformer (ViT) model proposed by Alexey Dosovitskiy et al. [cite].


\subsection{Residual layers}
In particular, we explore the use of residual learning techniques anddilated convolutions.


\begin{table}
	\centering
	\begin{tabular}{|l|l|l|l|}
    \hline
    name & residuals  & filters & total parameters \\
    \hline
    res3 & 3 & 19 & 20,380 \\
    \hline
    res6 & 6 & 19 & 40,330 \\
    \hline
    res9 & 9 & 19 & 60,280 \\
    \hline
    \end{tabular} 
\label{table:results}
\caption{Parameters of the ResNet models}
\end{table}

\subsection{Transformer layers}

Dosovitskiy et al. introduced the Vision Transformer (ViT) [2] and showed that Transformers can learn high-level image features by computing self-attention between different image patches. 	

Transformers were proposed by Vaswani et al. (2017) for machine translation, and have since become the state of the art method in many NLP tasks.
	 	 	 		
Alexey Dosovitskiy et al. created the ViT model for image classification using the Transformer architecture with self-attention to sequences of image patches, without using convolution layers.
	
 To apply this approach in our scenario, we split an audio signal into fixed\-size patches, linearly embed each of them, add position embeddings, and feed the resulting sequence of vectors to a standard Transformer encoder. In order to perform classification, we use the standard approach of adding an extra learnable "classification token" to the sequence.


An investigation into the application of the Transformer architecture to keyword spotting, finding that applying self-attention is more effective in the time domain than in the frequency domain.

\begin{table}
	\centering
	\begin{tabular}{|l|l|l|l|}
    \hline
    name & patches  & projection dim &  total parameters \\
    \hline
    vit5x4 & 5x4 & 20 & 20xxx \\
    \hline
    vit10x8 & 10x8 & 20 & 20xxx \\
    \hline
    vit20x10 & 20x10 & 20 & 20xxx \\
    \hline
    \end{tabular} 
\label{table:results}
\caption{Parameters of the Transformer models}
\end{table}


\subsection{Experimental Setup}

The Speech Commands Dataset wassplit into training, validation, and test sets, with 80\% training,10\% validation, and 10\% test.

To generate training data, we followed Google's preprocessingprocedure by adding background noise to eachsample with a probability of 0:8. The noise is chosen randomly from the background noises providedin the dataset. After evaluated different ratios of noise to add an every signal, we defined a ratio of 0.4 in order that a human ear can still listening the recorded commands with some background noise. 

For evaluate the models we use the accuracy metric and also we compute the total time used for training the models.

\subsection{Model Training}

We used stochastic gradient descent with a momentum of x and a starting learning rate of 0.01 for the ResNet model and 0.001 for the Transformer model, which is multiplied by x on plateaus. 

We used a mini-batch size of 64 and a total of 27 epochs to train our models.

Also, we add Early Stopping with patience 5 and for Plateau patience 2, and a min learning rate of 0.00001.
% !TEX root = template.tex

\section{Processing Pipeline}
\label{sec:processing_architecture}
\begin{figure}
\caption{Schematic}
\label{system_scheme}
\end{figure}

In a general way, the system used consists on the main components. An scheme of the system is shown on figure \ref{system_scheme}.  CNN is an image classification technique and one of the major challenges in speech and acoustic event recognition has been how to best represent the audio signal using an image for this purpose. Two common approaches have been seen in addressing this problem. Firstly, the audio signal is converted to spectrogram images \cite{zhang2015robust}. Secondly, a mel-filter, as used in computing mel-frequency cepstral coefficients (MFCC), is used to form an image-like representation \cite{abdel2014convolutional]}.  To ensure that all images are of an equal size, the audio signal is divided into a fixed number of frames \cite{We refer this as the mel-spectrogram.}. The first component consist on a feature extraction module  from the audio signals. The second module consists on the convolutional neural network architecture. This model will output a probability of each segment to belong to a class. Finally, a post-processing module, takes for each audio input the list of probabiltiies, and produces the final predicted class for that audio.

 



CNNs run a small window over the input image atboth training and testing time, so that the weights of the networkthat looks through this window canlearn from various featuresof the input data regardless of their absolute position within theinput.Weight sharing, or to be more precise in our present situ-ation,full weight sharingrefers to the decision to use the sameweights at every positioning of the window. CNNs are also often said to be local because the individual units that are computed at a particular positioning ofthe window depend upon featuresof the local region of the image that the window currently looksupon.
 Time presents no immediate problem from the standpoint of locality. Likeother DNNs for speech, a single window of input to the CNNwill consist of a wide amount of context (9–15 frames). Asfor frequency, the conventional use of MFCCs does present amajor problem because the discrete cosine transform projectsthe spectral energies into a new basis that may not maintain lo-cality. In this paper, we shall use the log-energy computed di-rectly from the mel-frequency spectral coefficients (i.e., with noDCT), which we will denote asMFSC features. These will beused to represent each speech frame in order to describethe acoustic energy distributionin each of several different frequency bands

There exist several different alternatives to organizing theseMFSC features into maps for the CNN. First, as shown inFig. 1(b), they can be arranged as three 2-D feature maps,each of which represents MFSC features (static, delta anddelta-delta) distributed along both frequency (using the fre-quency band index) and time (using the frame number withineach context window). In this case, a two-dimensional con-volution is performed (explained below) to normalize bothfrequency and temporal variations simultaneously. 


We evaluated our models using Google’s Speech CommandsDataset  [9],  which  was  released  in  August  2017  under  aCreative  Commons  license.2The  dataset  contains  65,000one-second long utterances of 30 short words by thousands ofdifferent people, as well as background noise samples such aspink noise, white noise, and human-made sounds.  The blogpost  announcing  the  data  release  also  references  Google’sTensorFlow implementation of Sainath and Parada’s models,which provide the basis of our comparisons


The output of each must be post-processed

\section{Signals and Features}
The audio signals For  feature  extraction,   we  first  apply  a  band-pass  filterof  20Hz/4kHz  to  the  input  audio  to  reduce  noise.    For the speech regions, we generate acoustic features based on40-dimensional log-filterbank energies computed every 10 ms overa window of 25 ms. Contiguous frames are stacked to add sufficientleft and right context. The input window is asymmetric since eachadditional frame of future context adds 10 ms of latency to the sys-tem. For our Deep KWS system, we use 10 future frames and 30frames in the past.



\section{Learning Framework}
 models  are  compared  against  three  CNN  variantsproposed by Sainath and Parada\cite{sainath2015convolutional}, named trad-fpool3, which is their base model   The accuracies of these models are shown in Table 4,which  also  shows  the  95%  confidence  intervals  from  fivedifferent optimization trials with different random seeds. Thetable  provides  the  number  of  model  parameters  as  well  asthe number of multiplies in an inference pass.   



\subsection{Residual layers}
In par-ticular, we explore the use of residual learning techniques anddilated convolutions.

\subsection{Attention layers}
\subsection{Inception-like layers}

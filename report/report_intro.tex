% !TEX root = template.tex

\section{Introduction}
\label{sec:introduction}
Automatic Speech Recognition (ASR) is a central part of Human-computer interfaces (HCI) that allows executing commands dictated by voice. The prevalence of mobile devices has caused an increase in the interest in the use of HCI commanded by voice. In the past years, due to the reduced computational power of mobile devices, the architecture of these interfaces consisted of transmitting the audio captured from the device to a more powerful server capable of doing ASR. After obtaining the decoded results, the information was transmitted to the user. This architecture may saturate the network, due to the fact that continuous audio streams from billions of these devices are sent to the cloud. Furthermore, such a cloud-based solution adds latency to the application, which may hurt user experience.  Also, there are privacy concerns when audio is transmitted outside the device owned by the user.


These challenges encouraged the research on replacing the server-based system with one that can run entirely on the device. One of the main approaches consists of detecting predefined keywords such as "Alexa", "Ok Google", "Hey Siri", etc., which is commonly known as keyword spotting (KWS). Detection of keyword wakes up the device and then activates the full-scale speech recognition either on the device \cite{mcgraw2016personalized} or in the cloud.  In some applications, the sequence of keywords can be used as voice commands to a smart device. Since KWS system is always on, it should have very low power consumption to maximize battery life. On the other hand, the KWS system should detect the keywords with high accuracy and low latency, for the best user experience.

In this context, diverse audio dataset of spoken words designed to help train and evaluate keyword spotting systems was published \cite{warden2018speech} and multiple supervised machine learning models models has been successfully applied in ASR to boost recognition accuracy \cite{yu2014automatic}. 
Convolutional Neural Networks (CNNs) have also become popular for acoustic modelling and have shown improvements over the fully connected feed-forward DNNs as KWS \cite{sainath2015convolutional} \cite{chen2014small}. An addition, convolutional models are suitable for running on hardware-limited devices since the size of the model can be easily controlled to fit the devices’ CPU and memory budget by changing the number of filter.  Compared to other deep learning architectures, CNN has several advantages: CNN is suited to exploit local correlations of human speech signals in both time and frequency dimensions and these networks have the capacity to exploit translational invariance in signals.


In this article, we analyze different convolutional neural network architectures to perform keyword spotting. The models were applied over MEL filters extracted on each song. We used the Google’s Speech Commands Dataset \cite{warden2018speech},  which  was  released in  August  2017 under a Creative  Commons license. The  dataset contains  65,000 one-second long utterances of 30 short words by thousands of different people, as well as background noise samples such as pink noise, white noise, and human-made sounds.  

The models analyzed consist 
We analyze adding residual layers on a well-defined architecture. Also, we add an attention layer over the features obtained with the convolutional masks. Finally, an inception-like architecture with multiscale convolutional layers was tested. Post-processing of the labels is performed. We report metrics of classification.


This article is structured as follow. In section II we describe the state of the art on KWS using convolutional neural network models. The architectures evaluated and the data used is presented on Section III. In 4 balbllab. The performance evaluation is prenseted on section VI. Finally, concluding remarks are provided in section VII.


% !TEX root = template.tex

\section{Introduction}
\label{sec:introduction}

Human-computer interfaces (HCI) like Google Assistant, Microsoft Cortana, Amazon Alexa, Apple Siri and others rely on Automatic Speech Recognition (ASR) to execute commands dictated by voice. In the past years, due to the reduced computational power of the mobile devices, the architecture of the application consisted on transmiting audio the audio captured from the device to a more powerful server capable of doing ASR. After obtaining the decoded results, the information were transmitted to the user. This architecture may cause network congestion to transmit continuous audio stream from billions of these devices to the cloud. Furthermore, such a cloud based solution adds latency to the application, which may hurt user experience.  Also, there are privacy concerns when audio is continuously transmitted to the cloud. 


These concerns encouraged the research on replacing the server-based system with one that can run entirely on-device. One of the main approaches consists on detecting predefined keyword(s) such as "Alexa", "Ok Google", "Hey Siri", etc., which is commonly known as keyword spotting (KWS). Detection of keyword wakes up the device and then activates the full scale speechrecognition either on device \cite{mcgraw2016personalized} or in the cloud.  In some applications, the sequence of keywords can be used as voice commands to a smart device. Since KWS system is always-on, it should have very low power consumption to maximize battery life. On theother hand, the KWS system should detect the keywords with high accuracy and low latency, for best user experience.

In the past years, deep learning \cite{yu2014automatic} has been successfully applied in ASR to boost the recognition accuracy.  A recent work investigated KWS based on a single-stage feed-forward  DNN . Such DNNs are also attractive forrunning on hardware-limited devices since the size of the modelcan be easily controlled to fit the devices’ CPU and memory  budget by changing the number of parameters in the DNN. Con-volutional Neural Networks (CNNs) have also become popularfor acoustic modeling and have shown improvements over thefully connected feed-forward DNNs as KWS \cite{sainath2015convolutional}. Compared to other deep learning architectures, CNN has several advantages:  1) CNN is suited to exploit local correlations of  human  speech  signals  in  both  time  and  frequency  di-mensions and these networks have the capacity to exploit translational invariance in signals.


In this article, we analyze differnet convolutional neural network architectures to perform keyword spotting. The models were applied over MEL filters extracted on each songs. We used the dataset pirulo pirulo. We analyze adding residual layers on a well defined archtiecture. Also we add a attention layer over the features obtained with the convolutional masks. Finally inception-like archicture with multiscale convolutional layers were tested. A post processing of the labels are performed. We report metrics of classification.
We evaluated our models using Google’s Speech CommandsDataset  [9],  which  was  released  in  August  2017  under  aCreative  Commons  license.2The  dataset  contains  65,000one-second long utterances of 30 short words by thousands ofdifferent people, as well as background noise samples such aspink noise, white noise, and human-made sounds.  The blogpost  announcing  the  data  release  also  references  Google’sTensorFlow implementation of Sainath and Parada’s models,which provide the basis of our comparisons

This article is structured as follow. In section II we describe the state of the art on KWS using convolutional. The architectures evaluated and the data used is presented on Section III. In 4 balbllab. The performance evaluation is prenseted on section VI. Finally, concluding remarks are provided in section VII.


% !TEX root = template.tex

\section{Related Work}
Keyword spotting is used to detect specific words from a stream of  audio,  typically  in  a  low-power  always-on  setting  such  as smart speakers and mobile phones.   To achieve this,  audio is processed locally on the device.  In addition to detecting target words,  classifiers may also distinguish between ``silence'' and ``unknown'' for words or sounds that are not in the target list. In  recent  years,   machine  learning  techniques,   such  asdeep (DNN), convolutional (CNN) and recurrent (RNN) neu-ral  networks,  have  proven  to  be  useful  for  keyword  spotting. These networks are typically used in conjunction with a pre-processing pipeline which extracts the mel-frequency cepstrumcoefficients (MFCC) \cite{davis1980comparison}. 


Deep learning models have shown state-of-the-art performance in KWS \cite{hinton2012deep}, \cite{sainath2013deep}. 



The authors in \cite{chen2014small} have proposed a low-latency keyword detection method for mobile users using a deep learning-based technique and termed it as ‘deep KWS’. The deep KWS method has not only been proven suitable for low-powered embedded systems but also has outperformed the baseline Hidden Markov Models for both noisy and noise-free audio data. The deep KWS uses a fully connected DNN with transfer learning  based on speech recognition. The network is further optimized for KWS with end-to-end fine-tuning using stochastic gradient descent. Sainath et al. \cite{sainath2015convolutional} have introduced a similarly small footprint KWS system based on CNNs. Their proposed CNN uses fewer parameters than a standard DNN model, which makes the proposed system more attractive for platforms with resource constraints. 
The DNN is a standard feed-forward neural network made of a stack of fully-connected layers andnon-linear activation layers. The input to the DNN is the flattened feature matrix, which feeds into astack ofdhidden fully-connected layers each withnneurons. Typically, each fully-connected layeris followed by a rectified linear unit (ReLU) based activation function. At the output is a linear layerfollowed by a softmax layer generating the output probabilities of thekkeywords, which are used forfurther posterior handling.3.2  Convolutional Neural Network (CNN)One main drawback of DNN based KWS is that they fail to efficiently model the local temporaland spectral correlation in the speech features. CNNs exploit this correlation by treating the inputtime-domain and spectral-domain features as an image and performing 2-D convolution operationsover it.  The convolution layers are typically followed by batch normalization [17], ReLU basedactivation functions and optional max/average pooling layers, which reduce the dimensionality ofthe features. During inference, the parameters of batch normalization can be folded into the weightsof the convolution layers. In some cases, a linear low-rank layer, which is simply a fully-connectedlayer without non-linear activation, is added in between the convolution layers and dense layers forthe purpose of reducing parameters and accelerating training  [18, 19].3.3  Recurrent Neural Network (RNN)RNNs have shown superior performance in many sequence modeling tasks, especially speech recogni-tion [20], language modeling  [21], translation [22], etc. RNNs not only exploit the temporal relationbetween the input signal, but also capture the long-term dependencies using "gating" mechanism.Unlike CNNs where input features are treated as 2-D image, RNNs operate forTtime steps, whereat each time steptthe corresponding spectral feature vectorft∈RFconcatenated with the previoustime step outputht−1is used as input to the RNN. Figure 2 shows the model architecture of a typicalRNN model, where the RNN cell could be an LSTM cell [23,24] or a gated recurrent unit (GRU)cell [25,26]. Since the weights are reused across all theTtime steps, the RNN models tend to haveless number of parameters compared to the CNNs. Similar to batch normalization in CNNs, researchshow that applying layer normalization can be beneficial for training RNNs [27], in which the hiddenstates are normalized during each time step

% !TEX root = template.tex

\section{Related Work}

Deep learning models have shown state-of-the-art performance in speech recognition \cite{hinton2012deep}, \cite{sainath2013deep}. Following the work in [28], DBNs are trained in layer-wise fashion followed by end-to-end fine-tuning for speech applications as shown in Fig. 2 above. The DBN architecture and training process have been extensively tested on several large-vocabulary speech recognition datasets including TIMIT, Bing-Voice-Search speech, Switchboard speech, Google Voice Input speech, YouTube speech, and the English-Broadcast-News speech data set. DBNs significantly outperform state-of-the-art methods in speech recognition when compared to highly tuned Gaussian mixture model (GMM)Hidden Markov Model (HMM). SAEs likewise are shown to outperform GMM-HMM on Cantonese and other speech recognition tasks [43].


The success of future speech and vision systems depends on accessibility and adaptability to a variety of platforms that eventually drive the prospect of commercialization. While some platforms are intended for public and personal use, there are other commercial, industrial, and online-based platforms - all of which require seamless integration of IS. However, state-of-the-art deep learning models have challenges in adapting to embedded hardware due to large memory footprint, high computational complexity, and high-power consumption. This has driven research on improving system performance of compact architectures in resource restricted platforms. The following sections highlight some of the major research efforts in integrating sophisticated algorithms in resource restricted user platforms.
4.1. Speech recognition on mobile platforms


The authors in \cite{chen2014small} have proposed a low-latency keyword detection method for mobile users using a deep learning-based technique and termed it as ‘deep KWS’. The deep KWS method has not only been proven suitable for low-powered embedded systems but also has outperformed the baseline Hidden Markov Models for both noisy and noise-free audio data. The deep KWS uses a fully connected DNN with transfer learning  based on speech recognition. The network is further optimized for KWS with end-to-end fine-tuning using stochastic gradient descent. Sainath et al. \cite{sainath2015convolutional} have introduced a similarly small footprint KWS system based on CNNs. Their proposed CNN uses fewer parameters than a standard DNN model, which makes the proposed system more attractive for platforms with resource constraints. 

In \cite{tang2018deep}

Similar to KWS systems, automatic speech recognition (ASR) [170] has become increasingly popular with mobile devices as it alleviates the need for tedious typing on small mobile devices. Google provides ASR-based search services [166] on Android, iOS, and Chrome platforms. Apple iOS devices are equipped with a conversational assistant named Siri. Mobile users can also type texts or emails by speech on both Android and iOS devices [171]. However, ASR service is contingent on the availability of cellular mobile network since the recognition task is performed on a remote server. This is a limitation since mobile network strength can be low, intermittent, or even absent at places. Therefore, developing an accurate speech recognition system in real-time, embedded on standalone modern mobile devices, is still an active area of research.




LSTM
Chen et al. [169] in another study propose the use of LSTM for the KWS task. The inherent recurrent connections in LSTM can make the KWS task suitable for resource restricted platforms by improving computational efficiency. To support this, the authors further show that the proposed LSTM outperforms a typical DNN-based KWS method. A typical framework for deep learning based KWS system is shown in Fig. 5.

